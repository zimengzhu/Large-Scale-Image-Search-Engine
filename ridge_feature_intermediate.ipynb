{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS5785 final\n",
    "## Xianhui Li, Zimeng Zhu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "import string\n",
    "\n",
    "num_train = 8000\n",
    "num_dev = 2000\n",
    "num_test = 2000\n",
    "split_idx = list(range(num_train + num_dev))\n",
    "random.shuffle(split_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_descriptions(data_dir, num_doc):\n",
    "    docs = []\n",
    "    for i in range(num_doc):\n",
    "        path = os.path.join(data_dir, \"%d.txt\" % i)\n",
    "        with open(path) as f:\n",
    "            docs.append(f.read())\n",
    "    return docs\n",
    "\n",
    "# build x matrices\n",
    "train_dev_desc = parse_descriptions(\"descriptions_train\", num_doc=(num_train+num_dev))\n",
    "test_desc = parse_descriptions(\"descriptions_test\", num_doc=num_test)\n",
    "x_train = np.array([train_dev_desc[i] for i in split_idx[:num_train]])\n",
    "x_dev = np.array([train_dev_desc[i] for i in split_idx[num_train:]])\n",
    "x_test = np.array([d for d in test_desc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built all y matrices!\n",
      "y_train shape: (8000, 2048)\n",
      "y_dev shape: (2000, 2048)\n",
      "y_test shape: (2000, 2048)\n"
     ]
    }
   ],
   "source": [
    "def parse_features(features_path):\n",
    "    vec_map = {}\n",
    "    with open(features_path) as f:\n",
    "        for row in csv.reader(f):\n",
    "            img_id = int(row[0].split(\"/\")[1].split(\".\")[0])\n",
    "            vec_map[img_id] = np.array([float(x) for x in row[1:]])\n",
    "    return np.array([v for k, v in sorted(vec_map.items())])\n",
    "\n",
    "# build y matrices\n",
    "p = np.random.randn(2048, 2048)\n",
    "y_train_dev = parse_features(\"features_train/features_resnet1000intermediate_train.csv\") @ p\n",
    "y_train = y_train_dev[split_idx[:num_train]]\n",
    "y_dev = y_train_dev[split_idx[num_train:]]\n",
    "y_test = parse_features(\"features_test/features_resnet1000intermediate_test.csv\") @ p\n",
    "\n",
    "print(\"Built all y matrices!\")\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_dev shape:\", y_dev.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocess x train dataset\n",
    "new_x_train=[]\n",
    "for i in x_train:\n",
    "    ## Lowercase all of the words\n",
    "    i = i.lower()\n",
    "    \n",
    "    ## strip punctuation\n",
    "    i = i.translate(str.maketrans('','',string.punctuation))\n",
    "    i = i.translate(str.maketrans('','','1234567890'))\n",
    "    \n",
    "    ## strip stop words\n",
    "    parse = i.split()\n",
    "    parse = [word for word in parse if word not in stopwords.words('english')]\n",
    "    \n",
    "    ## Lemmatization of all the words\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    parse = [lmtzr.lemmatize(a) for a in parse]\n",
    "    \n",
    "    new_x_train.append(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocess x_test dataset\n",
    "new_x_test=[]\n",
    "for i in x_test:\n",
    "    ## Lowercase all of the words\n",
    "    i = i.lower()\n",
    "    \n",
    "    ## strip punctuation\n",
    "    i = i.translate(str.maketrans('','',string.punctuation))\n",
    "    i = i.translate(str.maketrans('','','1234567890'))\n",
    "    \n",
    "    ## strip stop words\n",
    "    parse = i.split()\n",
    "    parse = [word for word in parse if word not in stopwords.words('english')]\n",
    "    \n",
    "    ## Lemmatization of all the words\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    parse = [lmtzr.lemmatize(a) for a in parse]\n",
    "    ##st = LancasterStemmer()\n",
    "    ##parse = [st.stem(a) for a in parse]\n",
    "    \n",
    "    new_x_test.append(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocess x_dev dataset\n",
    "new_x_dev=[]\n",
    "for i in x_dev:\n",
    "    ## Lowercase all of the words\n",
    "    i = i.lower()\n",
    "    \n",
    "    ## strip punctuation\n",
    "    i = i.translate(str.maketrans('','',string.punctuation))\n",
    "    i = i.translate(str.maketrans('','','1234567890'))\n",
    "    \n",
    "    ## strip stop words\n",
    "    parse = i.split()\n",
    "    parse = [word for word in parse if word not in stopwords.words('english')]\n",
    "    \n",
    "    ## Lemmatization of all the words\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    parse = [lmtzr.lemmatize(a) for a in parse]\n",
    "    ##st = LancasterStemmer()\n",
    "    ##parse = [st.stem(a) for a in parse]\n",
    "    \n",
    "    new_x_dev.append(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "x_train = []\n",
    "for i in range(len(new_x_train)):\n",
    "    x_train.append(' '.join(new_x_train[i]))\n",
    "\n",
    "x_train_bow = vectorizer.fit_transform(x_train)\n",
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "x_train_tfidf = transformer.fit_transform(x_train_bow).toarray()\n",
    "\n",
    "x_test = []\n",
    "for i in range(len(new_x_test)):\n",
    "    x_test.append(' '.join(new_x_test[i]))\n",
    "    \n",
    "vectorizer2 = CountVectorizer(vocabulary = vocab)\n",
    "x_test_bow = vectorizer2.fit_transform(x_test)\n",
    "x_test_tfidf = transformer.fit_transform(x_test_bow).toarray()\n",
    "\n",
    "x_dev = []\n",
    "for i in range(len(new_x_dev)):\n",
    "    x_dev.append(' '.join(new_x_dev[i]))\n",
    "\n",
    "x_dev_bow = vectorizer2.fit_transform(x_dev)\n",
    "x_dev_tfidf = transformer.fit_transform(x_dev_bow).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 7495)\n",
      "(2000, 7495)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x_train_tfidf.shape)\n",
    "print(x_dev_bow.shape)\n",
    "type(x_dev_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tags(data_dir, num_doc):\n",
    "    docs = []\n",
    "    for i in range(num_doc):\n",
    "        path = os.path.join(data_dir, \"%d.txt\" % i)\n",
    "        with open(path) as f:\n",
    "            docs.append(f.read())\n",
    "    return docs\n",
    "\n",
    "# build x matrices\n",
    "train_dev_tag = parse_tags(\"tags_train\", num_doc=(num_train+num_dev))\n",
    "test_tag = parse_tags(\"tags_test\", num_doc=num_test)\n",
    "z_train = np.array([train_dev_tag[i] for i in split_idx[:num_train]])\n",
    "z_dev = np.array([train_dev_tag[i] for i in split_idx[num_train:]])\n",
    "z_test = np.array([d for d in test_tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_z = CountVectorizer()\n",
    "z_train_bow = vectorizer_z.fit_transform(z_train)\n",
    "vocab = np.array(vectorizer_z.get_feature_names())\n",
    "z_train_tfidf = transformer.fit_transform(z_train_bow).toarray()\n",
    "\n",
    "vectorizer_z2 = CountVectorizer(vocabulary = vocab)\n",
    "z_test_bow = vectorizer_z2.fit_transform(z_test)\n",
    "z_test_tfidf = transformer.fit_transform(z_test_bow).toarray()\n",
    "\n",
    "z_dev_bow = vectorizer_z2.fit_transform(z_dev)\n",
    "z_dev_tfidf = transformer.fit_transform(z_dev_bow).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained linear regression model!\n",
      "Summary of best model:\n",
      "Ridge(alpha=10.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# train OLS model with regression\n",
    "parameters = {\"alpha\": [10.0]}\n",
    "reg = GridSearchCV(Ridge(), parameters)\n",
    "reg.fit(x_train_bow, y_train)\n",
    "reg_best = reg.best_estimator_\n",
    "\n",
    "print(\"Trained linear regression model!\")\n",
    "print(\"Summary of best model:\")\n",
    "print(reg_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development MAP@20: 0.28184251942373073\n",
      "Mean index of true image 32.328\n",
      "Median index of true image 7.0\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "def dist_matrix(x1, x2):\n",
    "    return sklearn.metrics.pairwise.cosine_distances(x1, x2)\n",
    "# def dist_matrix(x1, x2):\n",
    "#     return ((np.expand_dims(x1, 1) - np.expand_dims(x2, 0)) ** 2).sum(2) ** 0.5\n",
    "\n",
    "# test performance on development set\n",
    "y_dev_pred = reg.predict(x_dev_bow)\n",
    "dev_distances = dist_matrix(y_dev_pred, y_dev)\n",
    "dev_scores = []\n",
    "dev_pos_list = []\n",
    "\n",
    "for i in range(num_dev):\n",
    "    pred_dist_idx = list(np.argsort(dev_distances[i]))\n",
    "    dev_pos = pred_dist_idx.index(i)\n",
    "    dev_pos_list.append(dev_pos)\n",
    "    if dev_pos < 20:\n",
    "        dev_scores.append(1 / (dev_pos + 1))\n",
    "    else:\n",
    "        dev_scores.append(0.0)\n",
    "\n",
    "print(\"Development MAP@20:\", np.mean(dev_scores))\n",
    "print(\"Mean index of true image\", np.mean(dev_pos_list))\n",
    "print(\"Median index of true image\", np.median(dev_pos_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output written!\n"
     ]
    }
   ],
   "source": [
    "# create test predictions\n",
    "x_train_all = np.concatenate([x_train_bow.toarray(), x_dev_bow.toarray()])\n",
    "y_train_all = np.concatenate([y_train, y_dev])\n",
    "reg_best.fit(x_train_all, y_train_all)\n",
    "y_test_pred = reg_best.predict(x_test_bow.toarray())\n",
    "test_distances = dist_matrix(y_test_pred, y_test)\n",
    "pred_rows = []\n",
    "\n",
    "for i in range(num_test):\n",
    "    test_dist_idx = list(np.argsort(test_distances[i]))\n",
    "    top_20 = test_dist_idx[:20]\n",
    "    row = [\"%d.jpg\" % i for i in test_dist_idx[:20]]\n",
    "    pred_rows.append(\" \".join(row))\n",
    "\n",
    "with open(\"test_submission_f.csv\", \"w\") as f:\n",
    "    f.write(\"Descritpion_ID,Top_20_Image_IDs\\n\")\n",
    "    for i, row in enumerate(pred_rows):\n",
    "        f.write(\"%d.txt,%s\\n\" % (i, row))\n",
    "\n",
    "print(\"Output written!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
