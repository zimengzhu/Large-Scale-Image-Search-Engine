{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS5785 final\n",
    "## Xianhui Li, Zimeng Zhu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "import string\n",
    "\n",
    "num_train = 8000\n",
    "num_dev = 2000\n",
    "num_test = 2000\n",
    "split_idx = list(range(num_train + num_dev))\n",
    "random.shuffle(split_idx)\n",
    "COMPONMENT=100\n",
    "ITER=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_descriptions(data_dir, num_doc):\n",
    "    docs = []\n",
    "    for i in range(num_doc):\n",
    "        path = os.path.join(data_dir, \"%d.txt\" % i)\n",
    "        with open(path) as f:\n",
    "            docs.append(f.read())\n",
    "    return docs\n",
    "\n",
    "# build x matrices\n",
    "train_dev_desc = parse_descriptions(\"descriptions_train\", num_doc=(num_train+num_dev))\n",
    "test_desc = parse_descriptions(\"descriptions_test\", num_doc=num_test)\n",
    "x_train = np.array([train_dev_desc[i] for i in split_idx[:num_train]])\n",
    "x_dev = np.array([train_dev_desc[i] for i in split_idx[num_train:]])\n",
    "x_test = np.array([d for d in test_desc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tags(data_dir, num_doc):\n",
    "    docs = []\n",
    "    for i in range(num_doc):\n",
    "        path = os.path.join(data_dir, \"%d.txt\" % i)\n",
    "        with open(path) as f:\n",
    "            docs.append(f.read())\n",
    "    return docs\n",
    "\n",
    "# build x matrices\n",
    "train_dev_tag = parse_tags(\"tags_train\", num_doc=(num_train+num_dev))\n",
    "test_tag = parse_tags(\"tags_test\", num_doc=num_test)\n",
    "z_train = np.array([train_dev_tag[i] for i in split_idx[:num_train]])\n",
    "z_dev = np.array([train_dev_tag[i] for i in split_idx[num_train:]])\n",
    "z_test = np.array([d for d in test_tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocess x train dataset\n",
    "new_x_train=[]\n",
    "for i in x_train:\n",
    "    ## Lowercase all of the words\n",
    "    i = i.lower()\n",
    "    \n",
    "    ## strip punctuation\n",
    "    i = i.translate(str.maketrans('','',string.punctuation))\n",
    "    i = i.translate(str.maketrans('','','1234567890'))\n",
    "    \n",
    "    ## strip stop words\n",
    "    parse = i.split()\n",
    "    parse = [word for word in parse if word not in stopwords.words('english')]\n",
    "    \n",
    "    ## Lemmatization of all the words\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    parse = [lmtzr.lemmatize(a) for a in parse]\n",
    "    \n",
    "    new_x_train.append(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocess x_test dataset\n",
    "new_x_test=[]\n",
    "for i in x_test:\n",
    "    ## Lowercase all of the words\n",
    "    i = i.lower()\n",
    "    \n",
    "    ## strip punctuation\n",
    "    i = i.translate(str.maketrans('','',string.punctuation))\n",
    "    i = i.translate(str.maketrans('','','1234567890'))\n",
    "    \n",
    "    ## strip stop words\n",
    "    parse = i.split()\n",
    "    parse = [word for word in parse if word not in stopwords.words('english')]\n",
    "    \n",
    "    ## Lemmatization of all the words\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    parse = [lmtzr.lemmatize(a) for a in parse]\n",
    "    ##st = LancasterStemmer()\n",
    "    ##parse = [st.stem(a) for a in parse]\n",
    "    \n",
    "    new_x_test.append(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocess x_dev dataset\n",
    "new_x_dev=[]\n",
    "for i in x_dev:\n",
    "    ## Lowercase all of the words\n",
    "    i = i.lower()\n",
    "    \n",
    "    ## strip punctuation\n",
    "    i = i.translate(str.maketrans('','',string.punctuation))\n",
    "    i = i.translate(str.maketrans('','','1234567890'))\n",
    "    \n",
    "    ## strip stop words\n",
    "    parse = i.split()\n",
    "    parse = [word for word in parse if word not in stopwords.words('english')]\n",
    "    \n",
    "    ## Lemmatization of all the words\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    parse = [lmtzr.lemmatize(a) for a in parse]\n",
    "    ##st = LancasterStemmer()\n",
    "    ##parse = [st.stem(a) for a in parse]\n",
    "    \n",
    "    new_x_dev.append(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_to_01(x):\n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(x[0])):\n",
    "            x[i][j] = min(1, x[i][j]) \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "x_train = []\n",
    "for i in range(len(new_x_train)):\n",
    "    x_train.append(' '.join(new_x_train[i]))\n",
    "\n",
    "x_train_bow = vectorizer.fit_transform(x_train).toarray()\n",
    "x_train_bow = change_to_01(x_train_bow)\n",
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "x_test = []\n",
    "for i in range(len(new_x_test)):\n",
    "    x_test.append(' '.join(new_x_test[i]))\n",
    "    \n",
    "vectorizer2 = CountVectorizer(vocabulary = vocab)\n",
    "x_test_bow = vectorizer2.fit_transform(x_test)\n",
    "x_test_bow = x_test_bow.toarray()\n",
    "x_test_bow = change_to_01(x_test_bow)\n",
    "\n",
    "x_dev = []\n",
    "for i in range(len(new_x_dev)):\n",
    "    x_dev.append(' '.join(new_x_dev[i]))\n",
    "\n",
    "x_dev_bow = vectorizer2.fit_transform(x_dev).toarray()\n",
    "x_dev_bow = change_to_01(x_dev_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_z = CountVectorizer()\n",
    "z_train_bow = vectorizer_z.fit_transform(z_train).toarray()\n",
    "z_train_bow = change_to_01(z_train_bow)\n",
    "vocab = np.array(vectorizer_z.get_feature_names())\n",
    "#z_train_tfidf = transformer.fit_transform(z_train_bow).toarray()\n",
    "\n",
    "vectorizer_z2 = CountVectorizer(vocabulary = vocab)\n",
    "z_test_bow = vectorizer_z2.fit_transform(z_test).toarray()\n",
    "z_test_bow = change_to_01(z_test_bow)\n",
    "#z_test_tfidf = transformer.fit_transform(z_test_bow).toarray()\n",
    "\n",
    "z_dev_bow = vectorizer_z2.fit_transform(z_dev).toarray()\n",
    "z_dev_bow = change_to_01(z_dev_bow)\n",
    "#z_dev_tfidf = transformer.fit_transform(z_dev_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/cross_decomposition/pls_.py:77: UserWarning: Maximum number of iterations reached\n",
      "  warnings.warn('Maximum number of iterations reached')\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/cross_decomposition/pls_.py:313: UserWarning: X scores are null at iteration 92\n",
      "  warnings.warn('X scores are null at iteration %s' % k)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PLSRegression(copy=True, max_iter=100, n_components=100, scale=True,\n",
       "       tol=1e-06)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.linear_model import Ridge\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "# train OLS model with regression\n",
    "# parameters = {\"alpha\": [0.5, 1.0, 3.0]}\n",
    "# reg = GridSearchCV(Ridge(), parameters)\n",
    "pls = PLSRegression(n_components=COMPONMENT, max_iter=ITER)\n",
    "pls.fit(z_train_bow, x_train_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development MAP@20: 0.2563552649534847\n",
      "Mean index of true image 18.0385\n",
      "Median index of true image 8.0\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "def dist_matrix(x1, x2):\n",
    "    return sklearn.metrics.pairwise.cosine_distances(x1, x2)\n",
    "#     return ((np.expand_dims(x1, 1) - np.expand_dims(x2, 0)) ** 2).sum(2) ** 0.5\n",
    "\n",
    "# test performance on development set\n",
    "x_dev_pred = pls.predict(z_dev_bow)\n",
    "dev_distances = dist_matrix(x_dev_bow, x_dev_pred)\n",
    "dev_scores = []\n",
    "dev_pos_list = []\n",
    "\n",
    "for i in range(num_dev):\n",
    "    pred_dist_idx = list(np.argsort(dev_distances[i]))\n",
    "    dev_pos = pred_dist_idx.index(i)\n",
    "    dev_pos_list.append(dev_pos)\n",
    "    if dev_pos < 20:\n",
    "        dev_scores.append(1 / (dev_pos + 1))\n",
    "    else:\n",
    "        dev_scores.append(0.0)\n",
    "\n",
    "print(\"Development MAP@20:\", np.mean(dev_scores))\n",
    "print(\"Mean index of true image\", np.mean(dev_pos_list))\n",
    "print(\"Median index of true image\", np.median(dev_pos_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/cross_decomposition/pls_.py:77: UserWarning: Maximum number of iterations reached\n",
      "  warnings.warn('Maximum number of iterations reached')\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/cross_decomposition/pls_.py:313: UserWarning: X scores are null at iteration 92\n",
      "  warnings.warn('X scores are null at iteration %s' % k)\n"
     ]
    }
   ],
   "source": [
    "# create test predictions\n",
    "x_train_all = np.concatenate([x_train_bow, x_dev_bow])\n",
    "z_train_all = np.concatenate([z_train_bow, z_dev_bow])\n",
    "\n",
    "#pls2 = PLSRegression(n_components=COMPONMENT, max_iter=ITER)\n",
    "pls.fit(z_train_all, x_train_all)\n",
    "x_test_pred = pls.predict(z_test_bow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output written!\n"
     ]
    }
   ],
   "source": [
    "test_distances = dist_matrix(x_test_bow, x_test_pred)\n",
    "pred_rows = []\n",
    "\n",
    "for i in range(num_test):\n",
    "    test_dist_idx = list(np.argsort(test_distances[i]))\n",
    "    top_20 = test_dist_idx[:20]\n",
    "    row = [\"%d.jpg\" % i for i in test_dist_idx[:20]]\n",
    "    pred_rows.append(\" \".join(row))\n",
    "\n",
    "with open(\"xianhui.csv\", \"w\") as f:\n",
    "    f.write(\"Descritpion_ID,Top_20_Image_IDs\\n\")\n",
    "    for i, row in enumerate(pred_rows):\n",
    "        f.write(\"%d.txt,%s\\n\" % (i, row))\n",
    "\n",
    "print(\"Output written!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
